\documentclass{template}
\bibliographystyle{plainnat}
\input{preamble}
\begin{document}


\begin{center}
  {\Large \textbf{Microbiome Technical Report}}\\
  {\large Bryan Martin} \\ 
  {Draft Compiled: \today} 
\end{center}

\setcounter{section}{-1}

\section{Notation}
 \begin{table}[ht]
\centering
\begin{tabular}{l|c|c}
\textbf{Notation} & \textbf{Definition} & \textbf{Notes} \\
  \hline \hline
$\mbf{W}$ & raw count data & ($n\times Q+1$), observed\\
$\mbf{X}$ & compositional data & ($n\times Q+1$), latent\\
%d & $D-1$\\
$\mbf{y}$ & $\log(\mbf{x}_{-D}/\mbf{x}_{D})$ & distributed MVN, used for LN model\\
$\mbf{Y}$ & matrix with rows $\mbf{y}$ & ($n \times Q$), latent \\
$M_i$ & $\sum_{q=1}^{Q+1} (\mbf{W}_i)_q$ & ancillary
\end{tabular}
\end{table}

\section{Introduction}

Microbiome research, along with many other modern scientific research applications, requires methods to understand, interpret, and analyze compositional data. Standard statistical procedures are not reliable for these analyses due to problems arising from the unit-sum constraint, subcompositional analysis, and parameter interpretation (\cite{aitchison1986statistical}). These challenges motivate the additive logistic normal distribution (LN), created by \cite{aitchison1986statistical} for the purpose of analyzing compositional data.

The LN distribution models the log-ratio transformation of a composition as multivariate normal, thereby allowing for statistical procedures and tests based on multivariate normality, including tests for distributional fit. It can accommodate both dependent and independent structures, and independence can be tested as a hypothesis. Further, if a composition follows a LN distribution, then so does any subcomposition, subcomposition conditional on another subcomposition, permutation, and certain perturbations (\cite{aitchison1986statistical}). 

The additive logistic normal multinomial distribution (LNM, \cite{billheimer2001statistical}) combines the LN distribution with a conditional mutinomial model. The LNM distribution models observed counts with a multinomial distribution, where the underlying compositions are considered random variables modeled by the LN distribution. The LNM model can be used as a data-generating distribution for multivariate count data, while incorporating the flexible covariance structure of the LN distribution. 

The LNM model is the focus of  \cite{xia2013logistic}. They evaluate the performance of this model in variable selection. They use a group $\ell_1$ penalty to estimate LNM distribution parameters. However, while the LNM distribution has several appealing qualities, likelihood-based inference is difficult because there is no closed form log-likelihood function. This motivates the use and necessity of Monte Carlo EM (MCEM) algorithm. In this algorithm, the E-step includes a  Metropolis-Hastings (MH) algorithm to sample from unobserved compositions, and the M-step selects variables by maximizing a penalized estimation problem.

In this manuscript, we implement and evaluate the performance of the parameter estimation procedure of \cite{xia2013logistic}. Currently, we simplify by removing penalization and covariates. We apply the MCEM algorithm to Whitman soil field data.


\section{Model}
\subsection{Data Description and Notation}
Following the notation of Section 0, we use count data on $n=119$ samples that of $Q+1=7770$ OTUs. $\mbf{W}=(\mbf{W}_1,\ldots,\mbf{W}_{Q+1})^T\in\R^{n\times Q+1}$ denotes the random vector of counts. $M_i=\sum_{q=1}^{Q+1} (\mbf{W}_i)_q$ denotes the total count of OTU $i$. $\mbf{X}=(\mbf{X}_1,\ldots,\mbf{X}_{Q})^T\in\R^{n\times Q+1}$ denotes the underlying compositions, so that $\sum_{q=1}^{Q+1} (\mbf{X}_{i})_q=1$ for all OTUs $i$.

\subsection{The LNM Model}
We model raw counts $\mbf{W}$ conditional on the composition $\mbf{X}$ using a multinomial distribution
$$P(\mbf{W}|\mbf{X})\propto \prod_{q=1}^{Q+1} (\mbf{X}_q)^{\mbf{W}_q}.$$

The composition $\mbf{X}$ is modeled with an LN distribution to allow for a flexible covariance structure. To express this likelihood, we first apply the additive log-ratio transformation ($\phi$) to map $\mbf{x}$ to $\R^{Q}$. Thus, for arbitrary base $D$, we define the log-ratio transformed composition,
$$\mbf{Y}=\phi(\mbf{X})=\set{\log\paren{\dfrac{\mbf{X}_1}{\mbf{X}_D}},\ldots,\log\paren{\dfrac{\mbf{X}_{D-1}}{\mbf{X}_D}},\log\paren{\dfrac{\mbf{X}_{D+1}}{\mbf{X}_D}},\ldots,\log\paren{\dfrac{\mbf{X}_{Q+1}}{\mbf{X}_D}}}^T.$$
WLOG (\cite{aitchison1986statistical}), we can assume that $D=Q+1$. Thus,
$$\mbf{Y}=\phi(\mbf{X})=\set{\log\paren{\dfrac{\mbf{X}_1}{\mbf{X}_{Q+1}}},\ldots,\log\paren{\dfrac{\mbf{X}_{Q}}{\mbf{X}_{Q+1}}}}^T.$$
We then define the inverse function, $\phi^{-1}$,
\bal 
\mbf{X}_q=(\phi^{-1}(\mbf{Y}))_q&= \dfrac{\exp{\mbf{Y}_q}}{\sum_{q=1}^{Q}\exp{\mbf{Y}_q}+1}, \quad q=1,\ldots Q\\
\mbf{X}_{Q+1} &= \dfrac{1}{\sum_{q=1}^{Q}\exp{\mbf{Y}_q}+1}.
\eal 
$\mbf{Y}$ is then modeled using a multivariate normal distribution $\N_{Q}(\boldsymbol{\mu},\boldsymbol{\Sigma})$ with density
\begin{equation}\label{eq:fyeta}
f(\mbf{Y}; \boldsymbol{\mu},\boldsymbol{\Sigma}) \propto |\boldsymbol{\Sigma}|^{-1/2} \exp{-\dfrac{1}{2}(\mbf{Y}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mbf{Y}-\boldsymbol{\mu})}. \end{equation}
Finally, and importantly, this model is only well-defined when no proportions are equal to $0$. Thus, as in \cite{aitchison1986statistical}, \cite{billheimer2001statistical}, and \cite{xia2013logistic}, we slightly perturb composition $\mbf{X}$ before applying $\phi$.


\subsection{Conditional Distribution of Latent Variables}

The aim of using this distribution is the ability to solve for parameters $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ in order to inform us about our  compositions $\mbf{X}$ or $\mbf{Y}$. In doing so, we use our observed data $\mbf{W}$ to inform us about the distribution of latent $\mbf{Y}$. We denote the parameters of interest as $\boldsymbol{\eta}\equiv (\boldsymbol{\mu},\boldsymbol{\Sigma})$. Using the LNM model, we can write the conditional distribution
\bal  \pi(\mbf{Y}_i|\mbf{W}_i;\boldsymbol{\eta}^{(t-1)}) &\propto P(\mbf{W}_i|\mbf{Y}_i) \times f(\mbf{Y}_i|\boldsymbol{\eta}^{(t-1)})\eal 
The latter distribution is given by Equation~\eqref{eq:fyeta}. For the former, note that by the definition of $\phi^{-1}()$, we can fully recover $\mbf{X}_i$ given $\mbf{Y}_i$. Thus: 
\bal 
P(\mbf{W}_i|\mbf{Y}_i) &= P(\mbf{W}_i|\mbf{X}_i), \\ 
&= \prod_{q=1}^{Q+1} (X_{iq})^{W_{iq}},\\
&= X_{i,Q+1}^{W_{i,Q+1}}\prod_{q=1}^{Q} (\phi^{-1}(\mbf{Y}_i))_q^{W_{iq}},\\
&\propto \paren{\dfrac{ \exp{Y_{i1}}}{\brack{\sum_{q=1}^Q \exp{Y_{i1}}+1}}}^{W_{i1}} \times \cdots \times \paren{\dfrac{ \exp{Y_{iQ}}}{\brack{\sum_{q=1}^Q \exp{Y_{iQ}}+1}}}^{W_{iQ}}.
\eal
Thus, we arrive at 
\begin{equation}\label{eq:fullcond}
    \pi(\mbf{Y}_i|\mbf{W}_i;\boldsymbol{\eta}^{(t-1)}) \propto \dfrac{\prod_{q=1}^Q \exp{W_{iq}Y_{iq}}}{\brack{\sum_{q=1}^Q \exp{Y_{iq}}+1}^{M_i}} \paren{\exp{-\dfrac{1}{2}\brack{\mbf{Y}_i^{(t-1)*T}(\boldsymbol{\Sigma}^{(t-1)})^{-1}\mbf{Y}_i^{(t-1)*}}}},
\end{equation}
where $\mbf{Y}_i^{(t-1)*}\equiv \mbf{Y}_i- \boldsymbol{\mu}^{(t-1)}$.




\section{MC-EM Algorithm}



In general, the EM algorithm iterates between two steps. The E-step computes the \textit{expected complete data log-likelihood}. This is the correct log-likelihood if we take the current parameter estimates as given. The M-step \textit{updates parameter estimates} given the log-likelihood from the E-step. The estimates are then used for the E-step of the following iteration.



\noindent\textbf{E-Step}

The MC-EM algorithm complicates the standard EM algorithm by including MH in the E-step. This is necessary in this case because the conditional distribution is difficult to integrate, thus we cannot get the conditional expectation in closed form.

We aim to compute the expected complete data log-likelihood \bal 
E[\ell(\boldsymbol{\eta})]&\equiv Q(\boldsymbol{\eta}|\boldsymbol{\eta}^{(t-1)}),\\
&= -\dfrac{1}{2}n\log(|\boldsymbol{\Sigma}|)-\dfrac{1}{2}\sumi E\brack{(\mbf{Y}_i-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mbf{Y}_i-\boldsymbol{\mu})},
\eal 
where the expectation is taken with respect to the conditional distribution of $\mbf{Y}_i|(\mbf{W}_i; \boldsymbol{\eta}^{(t-1)})$. This conditional distribution is  given by Equation~\eqref{eq:fullcond}. We may rewrite the conditional expectation as
\bal 
E_{\pi(\mbf{Y}_i|(\mbf{W}_i; \boldsymbol{\eta}^{(t-1)}))}&\brack{(\mbf{Y}_i-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mbf{Y}_i-\boldsymbol{\mu})} = \\
&\int_{\Omega(\mbf{Y}_i)} (\mbf{Y}_i-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mbf{Y}_i-\boldsymbol{\mu})\pi(\mbf{Y}_i|(\mbf{W}_i; \boldsymbol{\eta}^{(t-1)})) d\mbf{Y}_i.
\eal 
Substituting in $\pi(\mbf{Y}_i|(\mbf{W}_i; \boldsymbol{\eta}^{(t-1)})) d\mbf{Y}_i$ from Equation~\eqref{eq:fullcond}, we can see the difficulty of solving this integral analytically. Thus, in order to estimate it numerically, we can use the Metropolis-Hastings algorithm. We simulate proposals $\mbf{Y}_i\sim \N(\mbf{Y}_i^{(r-1)},v\mbf{I})$, and after burn-ins, we use these $R$ samples to approximate our expectation.

\noindent\textbf{M-step}

Without covariates, updating to  $\boldsymbol{\eta}^{(t)}$ has closed form
\bal 
\boldsymbol{\Sigma}^{(t)} &=\dfrac{1}{R}\sum_{r=1}^{R}\paren{\dfrac{\sumi \paren{\mbf{Y}_i^{(r)*}}\paren{\mbf{Y}_i^{(r)*}}^T}{n}},\\
\boldsymbol{\mu}_{q}^{(t)} &= \onen\sumi \paren{\dfrac{1}{R}\sum_{r=1}^{R}Y_{iq}^{(r)}},
\eal 
where $\mbf{Y}^{(r)*}=\mbf{Y}_i^{(r)}-\boldsymbol{\mu}^{(t-1)}$.


\subsection{Pseudocode}

 I express the algorithm using 3 separate functions.


\FloatBarrier 
\begin{algorithm}[ht!]
\begin{algorithmic}[1]
\Input $\mbf{Y}_i\in\R^{Q}$, $\mbf{W}_i$, $E[\mbf{Y}_i]$, $D$, $\boldsymbol{\Sigma}^{-1}$, iter, $v$
\Output $\mbf{Y}^{MH}\in\R^{\text{iter}\times Q}$ (a MH sample matrix of $\mbf{Y}_i$)
\For{$j=1,\ldots,$ iter}
\State $\boldsymbol{\eps} \sim \N(0,v)$
\State Propose $\mbf{Y}_i^* \gets \mbf{Y}_i+\boldsymbol{\eps}$
\State $a \gets \min\paren{1,\dfrac{\pi(\mbf{Y}_i^*)}{\pi(\mbf{Y}_i)}}$
\State $u \sim \text{Uniform}(0,1)$
\If{$u<a$}
\State $\mbf{Y}^{MH}_{j,\cdot} \gets \mbf{Y}_i^*$
\Else 
\State $\mbf{Y}^{MH}_{j,\cdot} \gets \mbf{Y}_i$
\EndIf
\EndFor
\Return{$\mbf{Y}^{MH}$}
\end{algorithmic}
\caption{\texttt{MCrow}, Markov Resampling for a Single Row}
\end{algorithm}


\begin{algorithm}[ht!]
\begin{algorithmic}[1]
\Input $\mbf{Y}\in\R^{N\times Q}$, $\mbf{W}$, $E[\mbf{Y}]$, $D$, $\boldsymbol{\Sigma}^{-1}$, iter, $v$
\Output $\mbf{Y}^{MH}\in\R^{\text{iter}\times Q\times N}$ (a MH sample array of $\mbf{Y}$)
\For{$i=1,\ldots,$ iter}
\State $\mbf{Y}^{MH}_{\cdot,\cdot,i} \gets$ \texttt{MCrow}($\cdots$)
\EndFor
\Return{$\mbf{Y}^{MH}$}
\end{algorithmic}
\caption{\texttt{MCmat}, Markov Resampling for an Entire Matrix}
\end{algorithm}


\begin{algorithm}[ht!]
\begin{algorithmic}[1]
\Input  $\mbf{W}\in\R^{N\times Q+1}$, $D$,  iterEM, iterMC, burnEM, burnMC, $v$
\Output $\hat{\boldsymbol{\beta}}$, $\hat{\boldsymbol{\Sigma}}$
\State $\mbf{Y}\gets \texttt{logRatios}(\mbf{W},$ base $=D)$
\State $\boldsymbol{\beta}^{(0)} \gets \texttt{colMeans}(\mbf{Y})$
\State $E[\mbf{Y}]^{(0)} \gets \mbf{1}(\boldsymbol{\beta}^{(0)})^T$
\State $\boldsymbol{\Sigma}^{(0)} \gets \texttt{cov}(\mbf{Y}-E[\mbf{Y}^{(0)}])$
\For{em $=1:$iterEM}
\State $\mbf{A}\gets \texttt{MCmat}(\cdots)$
\State Burn the first burnMC iterations of $\mbf{A}$
\State $\mbf{Y}^*\gets$ means of $\mbf{A}$ across the remaining MC iterations
\State $\boldsymbol{\beta}^{(\text{em})} \gets \texttt{colMeans}(\mbf{Y}^*)$
\State $\mbf{S}\gets 0$
\For{$i=1:(\text{iterMC}-\text{burnMC})$}
\State $\mbf{e}\gets \mbf{A}_{i,\cdot,\cdot}^T-E[\mbf{Y}]^{(\text{em}-1)}$
\State $\mbf{S}\gets \mbf{S}+\mbf{e}^T\mbf{e}$
\EndFor
\State $\boldsymbol{\Sigma}^{(\text{em})}\gets \mbf{S}/(N\times (\text{iterMC}-\text{burnMC}))$
\State $E[\mbf{Y}]^{(\text{em})} \gets \mbf{1}(\boldsymbol{\beta}^{(\text{em})})^T$
\EndFor
\State Burn the first burnEM iterations of $\boldsymbol{\beta}$, $\boldsymbol{\Sigma}$
\State $\hat{\boldsymbol{\beta}}\gets$ mean of the remaining values in $\boldsymbol{\beta}$
\State $\hat{\boldsymbol{\Sigma}}\gets$ mean of the remaining values in $\boldsymbol{\Sigma}$
\end{algorithmic}
\caption{\texttt{MCEM}, Markov Chain EM Algorithm}
\end{algorithm}



\FloatBarrier

\newpage

\section{Preliminary Results}

The first figure is a replication of the figure from \cite{xia2013logistic}, except that now the results are portrayed on the compositional scale. Two models were fit, for \texttt{DayAmdmt} 11 and 21, respectively. The results are colored by model. Overall, it is difficult to compare models on this scale, though OTUs for which observed and fitted proportions were relatively high do suggest a meaningful difference between the groups.

\grapher{CombinedFittingGraph.pdf}{5in}

The second figure is an attempt to better quantify the differences observed across models in the first figure. This figure takes the expected, or fitted, compositional value from the \texttt{DayAmdmt} 11  model, and subtracts the corresponding value of the \texttt{DayAmdmt} 21. It shows more clearly that there are three OTU indices for which the expected difference  is quite substantial.

\grapher{ExpectedCompDiff.pdf}{5in}

The third figure displays the $\boldsymbol{\mu}$ with two sets of error bars for each model. The black error bars are the diagonal elements of $\boldsymbol{\Sigma}$. The red error bars are the observed standard deviation of the estimates of $\boldsymbol{\mu}$ across the EM iterations.

\grapher{errorBars.pdf}{5in}



\bibliography{bib}

\end{document}










\documentclass{template}
\bibliographystyle{plainnat}
\input{preamble}
\begin{document}


\begin{center}
  {\Large \textbf{Microbiome Technical Report}}\\
  {\large Bryan Martin} \\ 
  {Draft Compiled: \today} 
\end{center}

\setcounter{section}{-1}

\section{Notation}
 \begin{table}[ht]
\centering
\begin{tabular}{l|c|c}
\textbf{Notation} & \textbf{Definition} & \textbf{Notes} \\
  \hline \hline
$\mbf{W}$ & raw count data & ($n\times Q+1$), observed\\
$\mbf{X}$ & compositional data & ($n\times Q+1$), latent\\
%d & $D-1$\\
$\mbf{Y}_i$ & $\log(\mbf{X}_{i,-D}/\mbf{X}_{i,D})$ & distributed MVN, used for LN model\\
$\mbf{Y}$ & matrix with rows $\mbf{Y_i}$ & ($n \times Q$), latent \\
$M_i$ & $\sum_{q=1}^{Q+1} (\mbf{W}_i)_q$ & ancillary
\end{tabular}
\end{table}

\section{Introduction}

Microbiome research, along with many other modern scientific research applications, requires methods to understand, interpret, and analyze compositional data. Standard statistical procedures are not reliable for these analyses due to problems arising from the unit-sum constraint, subcompositional analysis, and parameter interpretation (\cite{aitchison1986statistical}). These challenges motivate the additive logistic normal distribution (LN), created by \cite{aitchison1986statistical} for the purpose of analyzing compositional data.

The LN distribution models the log-ratio transformation of a composition as multivariate normal, thereby allowing for statistical procedures and tests based on multivariate normality, including tests for distributional fit. It can accommodate both dependent and independent structures, and independence can be tested as a hypothesis. Further, if a composition follows a LN distribution, then so does any subcomposition, subcomposition conditional on another subcomposition, permutation, and certain perturbations (\cite{aitchison1986statistical}). 

The additive logistic normal multinomial distribution (LNM, \cite{billheimer2001statistical}) combines the LN distribution with a conditional mutinomial model. The LNM distribution models observed counts with a multinomial distribution, where the underlying compositions are considered random variables modeled by the LN distribution. The LNM model can be used as a data-generating distribution for multivariate count data, while incorporating the flexible covariance structure of the LN distribution. 

The LNM model is the focus of  \cite{xia2013logistic}. They evaluate the performance of this model in variable selection. They use a group $\ell_1$ penalty to estimate LNM distribution parameters. However, while the LNM distribution has several appealing qualities, likelihood-based inference is difficult because there is no closed form log-likelihood function. This motivates the use and necessity of Monte Carlo EM (MCEM) algorithm. In this algorithm, the E-step includes a  Metropolis-Hastings (MH) algorithm to sample from unobserved compositions, and the M-step selects variables by maximizing a penalized estimation problem.

In this manuscript, we implement and evaluate the performance of the parameter estimation procedure of \cite{xia2013logistic}. Currently, we simplify by removing penalization and covariates. We apply the MCEM algorithm to Whitman soil field data.


\section{Model}
\subsection{Data Description and Notation}
Following the notation of Section 0, we use count data on $n=119$ samples that of $Q+1=7770$ OTUs. $\mbf{W}=(\mbf{W}_1,\ldots,\mbf{W}_{Q+1})^T\in\R^{n\times Q+1}$ denotes the random vector of counts. $M_i=\sum_{q=1}^{Q+1} (\mbf{W}_i)_q$ denotes the total count of OTU $i$. $\mbf{X}=(\mbf{X}_1,\ldots,\mbf{X}_{Q})^T\in\R^{n\times Q+1}$ denotes the underlying compositions, so that $\sum_{q=1}^{Q+1} (\mbf{X}_{i})_q=1$ for all OTUs $i$.

\subsection{The LNM Model}
We model raw counts $\mbf{W}$ conditional on the composition $\mbf{X}$ using a multinomial distribution
$$P(\mbf{W}|\mbf{X})\propto \prod_{q=1}^{Q+1} (\mbf{X}_q)^{\mbf{W}_q}.$$

The composition $\mbf{X}$ is modeled with an LN distribution to allow for a flexible covariance structure. To express this likelihood, we first apply the additive log-ratio transformation ($\phi$) to map $\mbf{x}$ to $\R^{Q}$. Thus, for arbitrary base $D$, we define the log-ratio transformed composition,
$$\mbf{Y}=\phi(\mbf{X})=\set{\log\paren{\dfrac{\mbf{X}_1}{\mbf{X}_D}},\ldots,\log\paren{\dfrac{\mbf{X}_{D-1}}{\mbf{X}_D}},\log\paren{\dfrac{\mbf{X}_{D+1}}{\mbf{X}_D}},\ldots,\log\paren{\dfrac{\mbf{X}_{Q+1}}{\mbf{X}_D}}}^T.$$
WLOG (\cite{aitchison1986statistical}), we can assume that $D=Q+1$. Thus,
$$\mbf{Y}=\phi(\mbf{X})=\set{\log\paren{\dfrac{\mbf{X}_1}{\mbf{X}_{Q+1}}},\ldots,\log\paren{\dfrac{\mbf{X}_{Q}}{\mbf{X}_{Q+1}}}}^T.$$
We then define the inverse function, $\phi^{-1}$,
\bal 
\mbf{X}_q=(\phi^{-1}(\mbf{Y}))_q&= \dfrac{\exp{\mbf{Y}_q}}{\sum_{q=1}^{Q}\exp{\mbf{Y}_q}+1}, \quad q=1,\ldots Q\\
\mbf{X}_{Q+1} &= \dfrac{1}{\sum_{q=1}^{Q}\exp{\mbf{Y}_q}+1}.
\eal 
$\mbf{Y}$ is then modeled using a multivariate normal distribution $\N_{Q}(\boldsymbol{\mu},\boldsymbol{\Sigma})$ with density
\begin{equation}\label{eq:fyeta}
f(\mbf{Y}; \boldsymbol{\mu},\boldsymbol{\Sigma}) \propto |\boldsymbol{\Sigma}|^{-1/2} \exp{-\dfrac{1}{2}(\mbf{Y}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mbf{Y}-\boldsymbol{\mu})}. \end{equation}
Finally, and importantly, this model is only well-defined when no proportions are equal to $0$. Thus, as in \cite{aitchison1986statistical}, \cite{billheimer2001statistical}, and \cite{xia2013logistic}, we slightly perturb composition $\mbf{X}$ before applying $\phi$.


\subsection{Conditional Distribution of Latent Variables}

With this distribution,  we denote the parameters of interest as $\boldsymbol{\eta}\equiv (\boldsymbol{\mu},\boldsymbol{\Sigma})$. This is made difficult as they are parameters describing the distribution of latent $\boldsymbol{Y}$. Thus, in order to estimate our parameters of interest, we use our observed data $\mbf{W}$.  Using the LNM model, we can write the conditional distribution
\bal  \pi(\mbf{Y}_i|\mbf{W}_i;\boldsymbol{\eta}^{(t-1)}) &\propto P(\mbf{W}_i|\mbf{Y}_i) \times f(\mbf{Y}_i|\boldsymbol{\eta}^{(t-1)})\eal 
The latter distribution is given by Equation~\eqref{eq:fyeta}. For the former, note that by the definition of $\phi^{-1}()$, we can fully recover $\mbf{X}_i$ given $\mbf{Y}_i$. Thus: 
\bal 
P(\mbf{W}_i|\mbf{Y}_i) &= P(\mbf{W}_i|\mbf{X}_i), \\ 
&= \prod_{q=1}^{Q+1} (X_{iq})^{W_{iq}},\\
&= X_{i,Q+1}^{W_{i,Q+1}}\prod_{q=1}^{Q} (\phi^{-1}(\mbf{Y}_i))_q^{W_{iq}},\\
&= \paren{\dfrac{ 1}{\brack{\sum_{q=1}^Q \exp{Y_{iq}}+1}}}^{W_{i,Q+1}} \paren{\dfrac{ \exp{Y_{i1}}}{\brack{\sum_{q=1}^Q \exp{Y_{iq}}+1}}}^{W_{i1}} \times \cdots \\
&\quad \times \paren{\dfrac{ \exp{Y_{iQ}}}{\brack{\sum_{q=1}^Q \exp{Y_{iq}}+1}}}^{W_{iQ}}.
\eal
Thus, we arrive at 
\begin{equation}\label{eq:fullcond}
    \pi(\mbf{Y}_i|\mbf{W}_i;\boldsymbol{\eta}^{(t-1)}) \propto \dfrac{\prod_{q=1}^Q \exp{W_{iq}Y_{iq}}}{\brack{\sum_{q=1}^Q \exp{Y_{iq}}+1}^{M_i}} \paren{\exp{-\dfrac{1}{2}\brack{\mbf{Y}_i^{(t-1)*T}(\boldsymbol{\Sigma}^{(t-1)})^{-1}\mbf{Y}_i^{(t-1)*}}}},
\end{equation}
where $\mbf{Y}_i^{(t-1)*}\equiv \mbf{Y}_i- \boldsymbol{\mu}^{(t-1)}$.




\section{MC-EM Algorithm}

Ultimately, our goal is to estimate the complete data log-likelihood so that it can be maximized to solve for our parameters of interest. If $\mbf{Y}$ were observed,  then we could estimate $\hat{\boldsymbol{\eta}}$ using the known closed form solutions
\bal 
\hat{\boldsymbol{\mu}_i} &= \bar{\mbf{Y}_i},\\
\hat{\boldsymbol{\Sigma}} &= \onen \sumi (\mbf{Y}_i - \bar{\mbf{Y}_i})(\mbf{Y}_i-\bar{\mbf{Y}_i})^T.
\eal 
However, we cannot do this because we do not directly observe $\mbf{Y}$. Instead, we approximate the complete data log-likelihood using the EM algorithm.

In general, the EM algorithm iterates between two steps. The E-step computes the \textit{expected complete data log-likelihood}. This is the correct log-likelihood if we take the current parameter estimates as given. The M-step \textit{updates parameter estimates} given the log-likelihood from the E-step. The estimates are then used for the E-step of the following iteration.



\noindent\textbf{E-Step}

We aim to compute the expected complete data log-likelihood \bal 
E[\ell(\boldsymbol{\eta})]&\equiv Q(\boldsymbol{\eta}|\boldsymbol{\eta}^{(t-1)}),\\
&= -\dfrac{1}{2}n\log(|\boldsymbol{\Sigma}|)-\dfrac{1}{2}\sumi E\brack{(\mbf{Y}_i-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mbf{Y}_i-\boldsymbol{\mu})},
\eal 
where the expectation is taken with respect to the conditional distribution of $\mbf{Y}_i|(\mbf{W}_i; \boldsymbol{\eta}^{(t-1)})$. This conditional distribution is  given by Equation~\eqref{eq:fullcond}. We may rewrite the conditional expectation as
\bal 
E_{\pi(\mbf{Y}_i|(\mbf{W}_i; \boldsymbol{\eta}^{(t-1)}))}&\brack{(\mbf{Y}_i-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mbf{Y}_i-\boldsymbol{\mu})} = \\
&\int_{\Omega(\mbf{Y}_i)} (\mbf{Y}_i-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mbf{Y}_i-\boldsymbol{\mu})\pi(\mbf{Y}_i|(\mbf{W}_i; \boldsymbol{\eta}^{(t-1)})) d\mbf{Y}_i.\eal 
Substituting in $\pi(\mbf{Y}_i|(\mbf{W}_i; \boldsymbol{\eta}^{(t-1)}))$ from Equation~\eqref{eq:fullcond}, we can see the difficulty of solving this integral analytically. Thus, in order to estimate it numerically, we can use the Metropolis-Hastings (MH) algorithm. The MH algorithm allows us to approximate the conditional expectation integral above using simulations:
\bal 
E_{\pi(\mbf{Y}_i|(\mbf{W}_i; \boldsymbol{\eta}^{(t-1)}))}&\brack{(\mbf{Y}_i-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mbf{Y}_i-\boldsymbol{\mu})} = \\
&\approx \dfrac{1}{R} \sum_{r=1}^{R} (\mbf{Y}_i^{(r)}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mbf{Y}_i^{(r)}-\boldsymbol{\mu})\pi(\mbf{Y}_i^{(r)}|(\mbf{W}_i; \boldsymbol{\eta}^{(t-1)})),
\eal 
where $\mbf{Y}_i^{(r)}$ represent simulated values. 

 The MH algorithm allows us to approximate the conditional distribution by generating samples from  the known conditional posterior distribution. Instead of observed data, we use the average across the MH samples.


In the MH algorithm, we simulate proposals $\mbf{Y}_i\sim \N(\mbf{Y}_i^{(r-1)},v\mbf{I})$.  We then calculate the Metropolis acceptance ratio 
$$ r(\mbf{Y}_i|\mbf{Y}_i^{(r-1)}) = \min \paren{1,\dfrac{\pi(\mbf{Y}_i)}{\pi(\mbf{Y}_i^{(r-1)}}}.$$
We then simulate $u\sim \text{Uniform}(0,1)$. We accept our proposal $\mbf{Y}_i$ if $u\leq r(\mbf{Y}_i|\mbf{Y}_i^{(r-1)})$. Otherwise, we repeat the previously simulated value. We then burn some simulations, and use the remaining $R$ samples to approximate our expectation. The pseudocode of this process is shown in Algorithm 1, \texttt{MCrow}.




\noindent\textbf{M-step}

With MH samples,  maximizing $Q(\boldsymbol{\eta}|\boldsymbol{\eta}^{(t-1)})$ becomes equivalent to maximizing a standard MVN log-likelihood. The only difference is that instead of directly observing our samples, we approximate it from the output of the MH algorithm. This allows us to use the same closed form solutions, substituting in the average MH sample for the observed data matrix.

Thus, using our MH samples from the E-step to approximate the log-likelihood, updating to  $\boldsymbol{\eta}^{(t)}$ has closed form:
\bal 
\boldsymbol{\Sigma}^{(t)} &=\dfrac{1}{R}\sum_{r=1}^{R}\paren{\dfrac{\sumi \paren{\mbf{Y}_i^{(r)*}}\paren{\mbf{Y}_i^{(r)*}}^T}{n}},\\
\boldsymbol{\mu}_{q}^{(t)} &= \onen\sumi \paren{\dfrac{1}{R}\sum_{r=1}^{R}Y_{iq}^{(r)}},
\eal 
where $\mbf{Y}^{(r)*}=\mbf{Y}_i^{(r)}-\boldsymbol{\mu}^{(t-1)}$.


\subsection{Pseudocode}

 I express the algorithm using 3 separate functions.


\FloatBarrier 
\begin{algorithm}[ht!]
\begin{algorithmic}[1]
\Input $\mbf{Y}_i\in\R^{Q}$, $\mbf{W}_i$, $E[\mbf{Y}_i]$, $D$, $\boldsymbol{\Sigma}^{-1}$, iter, $v$
\Output $\mbf{Y}^{MH}\in\R^{\text{iter}\times Q}$ (a MH sample matrix of $\mbf{Y}_i$)
\For{$j=1,\ldots,$ iter}
\State $\boldsymbol{\eps} \sim \N(0,v)$
\State Propose $\mbf{Y}_i^* \gets \mbf{Y}_i+\boldsymbol{\eps}$
\State $a \gets \min\paren{1,\dfrac{\pi(\mbf{Y}_i^*)}{\pi(\mbf{Y}_i)}}$
\State $u \sim \text{Uniform}(0,1)$
\If{$u<a$}
\State $\mbf{Y}^{MH}_{j,\cdot} \gets \mbf{Y}_i^*$
\Else 
\State $\mbf{Y}^{MH}_{j,\cdot} \gets \mbf{Y}_i$
\EndIf
\EndFor
\Return{$\mbf{Y}^{MH}$}
\end{algorithmic}
\caption{\texttt{MCrow}, Markov Resampling for a Single Row}
\end{algorithm}


\begin{algorithm}[ht!]
\begin{algorithmic}[1]
\Input $\mbf{Y}\in\R^{N\times Q}$, $\mbf{W}$, $E[\mbf{Y}]$, $D$, $\boldsymbol{\Sigma}^{-1}$, iter, $v$
\Output $\mbf{Y}^{MH}\in\R^{\text{iter}\times Q\times N}$ (a MH sample array of $\mbf{Y}$)
\For{$i=1,\ldots,$ iter}
\State $\mbf{Y}^{MH}_{\cdot,\cdot,i} \gets$ \texttt{MCrow}($\cdots$)
\EndFor
\Return{$\mbf{Y}^{MH}$}
\end{algorithmic}
\caption{\texttt{MCmat}, Markov Resampling for an Entire Matrix}
\end{algorithm}


\begin{algorithm}[ht!]
\begin{algorithmic}[1]
\Input  $\mbf{W}\in\R^{N\times Q+1}$, $D$,  iterEM, iterMC, burnEM, burnMC, $v$
\Output $\hat{\boldsymbol{\beta}}$, $\hat{\boldsymbol{\Sigma}}$
\State $\mbf{Y}\gets \texttt{logRatios}(\mbf{W},$ base $=D)$
\State $\boldsymbol{\beta}^{(0)} \gets \texttt{colMeans}(\mbf{Y})$
\State $E[\mbf{Y}]^{(0)} \gets \mbf{1}(\boldsymbol{\beta}^{(0)})^T$
\State $\boldsymbol{\Sigma}^{(0)} \gets \texttt{cov}(\mbf{Y}-E[\mbf{Y}^{(0)}])$
\For{em $=1:$iterEM}
\State $\mbf{A}\gets \texttt{MCmat}(\cdots)$
\State Burn the first burnMC iterations of $\mbf{A}$
\State $\mbf{Y}^*\gets$ means of $\mbf{A}$ across the remaining MC iterations
\State $\boldsymbol{\beta}^{(\text{em})} \gets \texttt{colMeans}(\mbf{Y}^*)$
\State $\mbf{S}\gets 0$
\For{$i=1:(\text{iterMC}-\text{burnMC})$}
\State $\mbf{e}\gets \mbf{A}_{i,\cdot,\cdot}^T-E[\mbf{Y}]^{(\text{em}-1)}$
\State $\mbf{S}\gets \mbf{S}+\mbf{e}^T\mbf{e}$
\EndFor
\State $\boldsymbol{\Sigma}^{(\text{em})}\gets \mbf{S}/(N\times (\text{iterMC}-\text{burnMC}))$
\State $E[\mbf{Y}]^{(\text{em})} \gets \mbf{1}(\boldsymbol{\beta}^{(\text{em})})^T$
\EndFor
\State Burn the first burnEM iterations of $\boldsymbol{\beta}$, $\boldsymbol{\Sigma}$
\State $\hat{\boldsymbol{\beta}}\gets$ mean of the remaining values in $\boldsymbol{\beta}$
\State $\hat{\boldsymbol{\Sigma}}\gets$ mean of the remaining values in $\boldsymbol{\Sigma}$
\end{algorithmic}
\caption{\texttt{MCEM}, Markov Chain EM Algorithm}
\end{algorithm}



\FloatBarrier 
\clearpage
\section{Model Fit Plots}

Due to the increase in the number of model fitting plots, currently this section only contains code for \texttt{DayAmdmt} = 11.

\FloatBarrier

\begin{figure}[ht!]
    \centering
        \includegraphics[width=.75\textwidth]{Images/simpleMult11.pdf}
    \caption{These images display the observed composition on the X-axis and the estimated proportions under a simple multinomial model on the Y-axis. The red lines represent the estimated variance of the estimated proportions. Clearly, this model does allow for an appropriate variance model. }\label{fig:simpMult}
\end{figure}



\clearpage

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.38\textheight}
        \includegraphics[width=\textwidth]{Images/XiaPlotBars11.pdf}
        \caption{Observed compositional fit vs model estimate.}
        %\label{fig:gull}
    \end{subfigure}
\\
    \begin{subfigure}[b]{0.38\textheight}
        \includegraphics[width=\textwidth]{Images/XiaPlotBars11T.pdf}
        \caption{Observed compositional fit vs model estimate, by OTU.}
        %\label{fig:tiger}
    \end{subfigure}
    \caption{The first image displays the observed composition on the X-axis and $\phi^{-1}(\hat{\mu}_j)$ on the Y-axis. The latter plot separates the data by OTU index. The estimated composition values are constant within columns. The red bars represent the empirical 95\% confidence interval for each estimated composition based on 10,000 simulations from the model fit using the full $\hat{\boldsymbol{\Sigma}}$.}\label{fig:compFitBar}
\end{figure}

\clearpage 

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.38\textheight}
        \includegraphics[width=\textwidth]{Images/XiaPlotBars11diag.pdf}
        \caption{Observed compositional fit vs model estimate, using a diagonal covariance.}
        %\label{fig:gull}
    \end{subfigure}
\\
    \begin{subfigure}[b]{0.38\textheight}
        \includegraphics[width=\textwidth]{Images/XiaPlotBars11diagT.pdf}
        \caption{Observed compositional fit vs model estimate, by OTU, using a diagonal covariance.}
        %\label{fig:tiger}
    \end{subfigure}
    \caption{The first image displays the observed composition on the X-axis and $\phi^{-1}(\hat{\mu}_j)$ on the Y-axis. The latter plot separates the data by OTU index. The estimated composition values are constant within columns. The red bars represent the empirical 95\% confidence interval for each estimated composition based on 10,000 simulations from the model fit using only the diagonal of $\hat{\boldsymbol{\Sigma}}$.}\label{fig:compFitBarDiag}
\end{figure}

\clearpage 

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.38\textheight}
        \includegraphics[width=\textwidth]{Images/XiaPlotBars11small.pdf}
        \caption{Observed compositional fit vs model estimate, using a small diagonal covariance.}
        %\label{fig:gull}
    \end{subfigure}
\\
    \begin{subfigure}[b]{0.38\textheight}
        \includegraphics[width=\textwidth]{Images/XiaPlotBars11smallT.pdf}
        \caption{Observed compositional fit vs model estimate, by OTU, using a small diagonal covariance.}
        %\label{fig:tiger}
    \end{subfigure}
    \caption{The first image displays the observed composition on the X-axis and $\phi^{-1}(\hat{\mu}_j)$ on the Y-axis. The latter plot separates the data by OTU index. The estimated composition values are constant within columns. The red bars represent the empirical 95\% confidence interval for each estimated composition based on 10,000 simulations from the model fit using only a diagonal matrix with diagonal values equal to $10^{-5}$ for $\hat{\boldsymbol{\Sigma}}$.}\label{fig:compFitBarSmall}
\end{figure}

\clearpage 

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.38\textheight}
        \includegraphics[width=\textwidth]{Images/XiaPlotBars295v5.pdf}
        \caption{Observed compositional fit vs model estimate, using the five most frequent OTUs, and 295 randomly selected OTUs.}
        %\label{fig:gull}
    \end{subfigure}
\\
    \begin{subfigure}[b]{0.34\textheight}
        \includegraphics[width=\textwidth]{Images/XiaPlotBars295v5T.pdf}
        \caption{Observed compositional fit vs model estimate, using the five most frequent OTUs, and 295 randomly selected OTUs, by OTU index.}
        %\label{fig:tiger}
    \end{subfigure}
    \caption{The first image displays the observed composition on the X-axis and $\phi^{-1}(\hat{\mu}_j)$ on the Y-axis, with the five most frequent OTUs and 295 randomly selected OTUs. The latter plot separates the data by OTU index. The estimated composition values are constant within columns. The red bars represent the empirical 95\% confidence interval for each estimated composition based on 10,000 simulations from the model fit using $\hat{\boldsymbol{\Sigma}}$.}\label{fig:compFitBar295}
\end{figure}

\clearpage 

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.38\textheight}
        \includegraphics[width=\textwidth]{Images/XiaPlotBars300.pdf}
        \caption{Observed compositional fit vs model estimate, using 300 randomly selected OTUs.}
        %\label{fig:gull}
    \end{subfigure}
\\
    \begin{subfigure}[b]{0.38\textheight}
        \includegraphics[width=\textwidth]{Images/XiaPlotBars300T.pdf}
        \caption{Observed compositional fit vs model estimate, using 300 randomly selected OTUs, by OTU index.}
        %\label{fig:tiger}
    \end{subfigure}
    \caption{The first image displays the observed composition on the X-axis and $\phi^{-1}(\hat{\mu}_j)$ on the Y-axis, with 300 randomly selected OTUs. The latter plot separates the data by OTU index. The estimated composition values are constant within columns. The red bars represent the empirical 95\% confidence interval for each estimated composition based on 10,000 simulations from the model fit using $\hat{\boldsymbol{\Sigma}}$.}\label{fig:compFitBar300}
\end{figure}

\FloatBarrier
\clearpage 


\bibliography{bib}

\end{document}









\begin{comment}
\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.40\textheight}
        \includegraphics[width=\textwidth]{Images/XiaPlotWBars11.pdf}
        \caption{DayAmdmt = 11}
        %\label{fig:gull}
    \end{subfigure}
\\
    \begin{subfigure}[b]{0.40\textheight}
        \includegraphics[width=\textwidth]{Images/XiaPlotWBars21.pdf}
        \caption{DayAmdmt = 21}
        %\label{fig:tiger}
    \end{subfigure}
    \caption{These images display the square root of the observed counts $\mbf{W}$ on the X-axis and $\sqrt{n_i\phi^{-1}(\hat{\mu}_j)}$ on the Y-axis. The red dots represent the empirical 95\% confidence interval for each estimated point based on 10,000 simulations from the model fit. The larger points represent observed sample values that were outside of the estimated 95\% confidence interval. }\label{fig:FitBar}
\end{figure}
\end{comment}